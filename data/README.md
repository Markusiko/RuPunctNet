# Описание данных

Для моделирования были собраны данные сразу из нескольких источников. 

## Художественные произведения.
С сайта [avidreaders.ru](https://avidreaders.ru) были вручную скачаны и очищены (от некоторых непредсказуемых строк в начале и от сносок в конце) 16 книг в формате `.txt`. Такие исходные тексты (можно найти в `raw_data/books/`) были обработаны с помощью скрипта `filter_books.ipynb` (обработанные тексты можно найти в `prepared_data/books/`) и затем размечены с помощью файла `get_markup_books.ipynb`. Всего таким образом было получено 47 756 абзацев, содержащих 1 993 736 меток (1 592 668 `o`, 262 814 `,`, 107 422 `.`,  16 188 `!`, 14 644 `?`). Итоговая разметка по книгам записана в файл `books_markup.csv` (размер позволил выложить файл в репозиторий).

## Википедия
С помощью библиотеки [`wikiextractor`](https://github.com/attardi/wikiextractor]) были обработаны все статьи на русскоязычной Википедии. Получение сырых текстов статей производилось с помощью следующих терминальных команд:

```@python
git clone https://github.com/attardi/wikiextractor.git
cd wikiextractor
wget http://dumps.wikimedia.org/ruwiki/latest/ruwiki-latest-pages-articles.xml.bz2
python -m wikiextractor.WikiExtractor ruwiki-latest-pages-articles.xml.bz2
```
   Таким образом былы получены тексты 4629511 статей. Общее время сбора данных составило где-то 12 часов с учетом загрузки дампа медленным инетом автора. Сырые тексты были сохранены в папке (папка из папок текстшников: по 100 в каждой) общим весом 9 гигабайт. Тексты были обработаны с помощью скрипта `prepare_wiki.ipynb` и загружены в `.zip` на гугл диск [сюда](https://drive.google.com/file/d/1buokuh3h01KwE5iobVantH6YF_yR0vyX/view?usp=sharing). Пока что непонятно, какой объем данных мы далее возьмем для обучения из Википедии (также полагаю, необходимы дополнительные проверки текстов на мусорность). Следовательно, разметка по Вики в каком-либо файле пока что представлена не была (сделать ее несложно, все аналогично коду в `get_markup_books.ipynb`). Однако, для полноты картины были посчитаны метрики по всем текстам внутри обработанного датасета (по-прежнему файл `prepare_wiki.ipynb`):
   
| Показатель| Значение |
|-|-|
| Абзацы|14 813 389|
| Метки| 555 674 748|
| o|474 669 460|
| .| 40 085 706|
| ,|40 846 537 |
| !| 39 084|
|?|33 961|

Данных, действительно, удалось собрать много (но, как было сказано ранее, их качество было проверено лишь поверхностно, методом взгляда). Наиболее распространенный недочет данных Вики: особые символы в кодировке Unicode. При обучении предлагается выбрасывать такие тексты.


